\chapter{Framework}
\section{Introduction}
Integer linear programming (ILP) is a powerful mathematical framework for modeling and solving optimization problems with discrete decision variables. In recent decades, ILPs have found widespread applications in diverse fields such as operations research, logistics, telecommunications, finance, and bioinformatics. The ability to precisely model real-world problems using ILPs allows for the formulation and solution of complex optimization challenges, facilitating decision-making processes and resource allocation in various domains.

In [Insert Book], a novel approach to solving certain types of ILPs was introduced, demonstrating that specific ILP instances could be efficiently solved by transforming them into instances of the shortest path problem on a directed and weighted graph. This approach involved constructing a graph representation from the ILP constraints and finding the shortest path between two specially designated vertices. Notably, the algorithm presented in [Insert Book] exhibited polynomial runtime complexity when the dimension $m$ of the ILP was fixed.

The focus of this thesis is to revisit and extend this approach to a broader class of ILP instances. Specifically, we will consider ILPs of the form $A\vec x = \vec b$, where $A\in \Z^{m \times n}$,  $\vec x \in \N^n$ and $\vec b \in \N^m$. Unlike the ILPs discussed in [Insert Book], which required all columns of the matrix $A$ to sum up to the same number, we will relax this constraint and allow for different column sums in $A$, provided that they are strictly positive.

To achieve this, we will first translate the given ILP into a form that can be effectively handled by the algorithm proposed in [Insert Book]. This translation process will involve converting the original ILP constraints into a graph representation suitable for shortest path computation. Importantly, this translation introduces a degree of freedom that will significantly influence the performance and effectiveness of the final algorithm.

The main objective of this thesis is to explore and exploit this degree of freedom to enhance the performance and efficiency of the algorithm for solving ILPs with varying column sums. Specifically, we will investigate various strategies for utilizing the additional flexibility introduced by the relaxed constraints to improve the scalability, robustness, and solution quality of the algorithm.

\section{Definitions and Basic Statements}
\subsection{Notation}
In this thesis, I will stick to some conventions regarding notating vectors and matricies. To not confuse the readers, I'll quickly note them here:
\begin{itemize}
    \item All variables representing a vector, will have a vector arrow on top of them, e.g. $\vec b = (2, -4, 0)^\top$.
    \item To access certain entries, the position of that entry will be written as a index and the arrow will be discarded, e.g. $b_2 = -4$.
    \item An indexed familiy of vectors, will be notated with indeces and vector arrows, e.g. $\vec a_1 = (0, 1)^\top, \vec a_2 = (2, 3)^\top$.
    \item To access elements in indexed vectors, brackets will be used, e.g. $(\vec a_2)_1 = 2$.
    \item $\sprod{\cdot}{\cdot}$ will notate the standard dot-product, e.g. $\sprod{\vec a_1}{\vec a_2} = 3$.
    \item $\hat e_i$ will notate the $i$-th standard basis vector. The number of dimension will be known from context. e.g. $\hat e_2 = (0, 1, 0, 0)^\top$
    \item Vectors will always named with a lowercase letter, matricies with an uppercase letter.
    \item For the solution space of a system of linear equations $A\vec x = \vec b$ I will use the symbol $\solspace(A, \vec b) := \{\vec x\mid A\vec x = \vec b\}$
\end{itemize}
\subsection{Affine spaces}
% Define affine space: https://mathworld.wolfram.com/AffineSpace.html
\begin{definition}
    Let $V$ be a vector space over a field $\mathbb{K}$, and let $A$ be a nonempty set. Now define addition $p+\vec a$ in $A$ for any vector $\vec a$ in $V$ and element $p$ in $A$ subject to the conditions:
    \begin{enumerate}
        \item $p + \vec0 = p$
        \item $(p+\vec a)+\vec b=p+(\vec a+\vec b)$
        \item For any $q$ in $A$, there exists a unique vector $\vec a$ in $V$ such that $q=p+\vec a$.
    \end{enumerate}
    Here, $\vec a, \vec b$ in $V$. Note that (1) is implied by (2) and (3). Then $A$ is an \textbf{affine space}.
\end{definition}

For us, any affine space will be a subset of a vector space. Hence, I will also notate all elements in affine spaces with a vector arrow. 

\begin{definition}
    Let $V$ be a vector space over the field $\mathbb{K}$. A linear combination $\lambda_1 \vec a_1 + \dots + \lambda_r \vec a_r$, $\lambda_j \in \mathbb{K}$, $\vec a_j \in V$ is called an \textbf{affine combination} iff $\lambda_1 + \dots + \lambda_r = 1$.
\end{definition}
    
\begin{definition}
    Let $V$ be a vector space over the field $\mathbb{K}$ and $\vec a_1, \dots, \vec a_r \in V$. The \textbf{affine hull} $\aff(\vec a_1, \dots, \vec a_r)$ if the affine space, spanned by those vectors (with affine combinations), namely:
    $$\aff(\vec a_1, \dots, \vec a_r) = \{\lambda_1 \vec a_1 + \dots + \lambda_r \vec a_r \mid \lambda_j \in \mathbb{K}, \lambda_1 + \dots + \lambda_r = 1\}$$
\end{definition}
\begin{definition}
    Let $V$ be a vector space over the field $\mathbb{K}$. A family of vectors $\vec a_1, \dots, \vec a_r \in V$ are called \textbf{affinely independent} of the set $\{\vec a_2 - \vec a_1, \dots, \vec a_r - \vec a_1\}$ is linearly independent.
\end{definition}
% Affine independent vectors yield unique affine dependence
\begin{observation}
    Let $\aff(\vec a_1, \dots, \vec a_r)$ be some affine hull, then for any vector $\vec v \in \aff(\vec a_1, \dots, \vec a_r)$ it will hold that, the affine combination respresnting $\vec b$ is unique iff $\vec a_1, \dots, \vec a_r$ are affinely independent.
\end{observation}

\subsection{Convex Hull}
\begin{definition}
    \label{def:convex_hull}
    Let $V$ be a vector space over an ordered field $\mathbb{K}$. A linear combination $\lambda_1 \vec a_1 + \dots + \lambda_r \vec a_r$, $\lambda_j \in \mathbb{K}$, $\vec a_j \in V$ is called an \textbf{convex combination} iff it is affine (namely $\lambda_1 + \dots + \lambda_r = 1$) and $\forall j\in[r]\mid \lambda_j \geq 0$.
\end{definition}
\begin{definition}
    Let $V$ be a vector space over an ordered field $\mathbb{K}$ and $\vec a_1, \dots, \vec a_r \in V$. The \textbf{convex hull} $\conv(\vec a_1, \dots, \vec a_r)$ is the set of all vectors reachable by convex combinations, namely:
    $$\conv(\vec a_1, \dots, \vec a_r) = \{\lambda_1 \vec a_1 + \dots + \lambda_r \vec a_r \mid \lambda_j \in \mathbb{K}, \lambda_j \geq 0, \lambda_1 + \dots + \lambda_r = 1\}$$
\end{definition}
\begin{observation}
    The convex hull $\conv(\vec a_1, \dots, \vec a_r)$ is not only the set of all comvex combinations, but also the smallest convex set containing $\vec a_1, \dots, \vec a_r$.
\end{observation}

\subsection{Linear Programs (LPs)}
\begin{definition}
    A \textbf{linear program (LP)} is a mathematical optimization problem formulated as follows: Let $A\in\Z^{m\times n}, \vec b \in \Z^m, \vec\omega \in \Z^n$ be given.
    \begin{align*}
        \text{Find a vector:} & \quad \vec x \in \Q^n_{\geq 0} \\
        \text{That minimizes:} & \quad \sprod{\vec \omega}{\vec x}\\
        \text{Subject to:} & \quad A\vec x \leq \vec b \\
    \end{align*}
    This way of formulating a linear program is called the \textbf{standart form}.
\end{definition}

\begin{observation}
    \label{obs:lp_slack_form}
    Any linear program in standart form can be rewritten in its \textbf{slack form}. Let $A, \vec b, \vec\omega$ be defining matrix and vectors of an LP. Then $\vec\omega' := (\omega_1, \dots, \omega_n, 0, \dots, 0)$, $\vec b' = \vec b$ and $A' := (A\mid\imat_m)$ will define another LP as follows:
    \begin{align*}
        \text{Find a vector:} & \quad \vec x' \in \Q^{n+m}_{\geq 0} \\
        \text{That minimizes:} & \quad \sprod{\vec \omega'}{\vec x'}\\
        \text{Subject to:} & \quad A\vec x' = \vec b' \\
    \end{align*}
    The solution $\vec x'$ to that linear program will yield the solutions to the original linear program by $\vec x = (x'_1, \dots, x'_n)$. The free variables $x'_{n+1}, \dots, x'_{n+m}$ are called \textbf{slack variables}. This means, we can always understand an LP as a continuous minimisation of a solution space of some system of linear equations.
\end{observation}

\textbf{Note}: Linear programming is a well-studied problem in optimization theory and operations research. Due to their fundamental importance and wide-ranging applications, linear programs have been extensively researched, leading to the development of efficient algorithms for their solution. Notably, algorithms such as the simplex method and interior point methods are established techniques that are capable of solving linear programs efficiently, even for large-scale problem instances.

\subsection{Integer Linear Programs (ILPs)}
Integer linear programs are on the first glace very similar to linear programs (LP). The only difference is that the solution must have whole number entries. This nuance though, has drastic implication on its solving techniques. But let's first define them.

\begin{definition}
    An \textbf{integer linear program (LLP)} is a mathematical optimization problem formulated as follows: Let $A\in\Z^{m\times n}, \vec b \in \Z^m, \vec\omega \in \Z^n$ be given.
    \begin{align*}
        \text{Find a vector:} & \quad \vec x \in \N^n \\
        \text{That minimizes:} & \quad \sprod{\vec \omega}{\vec x}\\
        \text{Subject to:} & \quad A\vec x \leq \vec b \\
    \end{align*}
    This way of formulating a linear program is called the \textbf{standart form}.
\end{definition}
\begin{observation}
    Similar to observation \ref{obs:lp_slack_form}, we can also define a slack form for ILPs. This way of understanding ILPs is key to this thesis, as for us an ILP will always have a system of linear equation at its core. Let's define a more compact notation
\end{observation}

\begin{definition}
    The tuple $(A\in\Z^{m\times n}, \vec b \in \Z^m, \vec\omega \in \Z^n)$ will from now on define the following ILP in its slack from:
    \begin{align*}
        \text{Find a vector:} & \quad \vec x \in \N^n \\
        \text{That minimizes:} & \quad \sprod{\vec \omega}{\vec x}\\
        \text{Subject to:} & \quad A\vec x = \vec b \\
    \end{align*}
\end{definition}

While linear programs can often be efficiently solved using well-established algorithms, the introduction of integer constraints significantly increases the computational complexity. In general, determining the optimal solution to an integer linear program is an NP-complete problem, meaning that there is no known polynomial-time algorithm that can solve all instances of the problem. As a result, solving integer linear programs can be exceptionally challenging, requiring the use of specialized algorithms and heuristics to find approximate or optimal solutions within a reasonable amount of time. 