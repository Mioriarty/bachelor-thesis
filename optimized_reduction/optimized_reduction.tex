\chapter{Generalisation and Optimisation}
\section{Problem statement}
In chapter 2 we saw how to solve an ILP with a CCS matrix using the shortest path framework. Here we want to extend this method to non-CCS matricies and hence create a more general notion of solving ILPs. First this generalisation will yield a solving technique for general matricies over the netural numbers and later will be further generalized to matricies over the whole numbers who's column sums are strictly positive. 

This generalisation will be done by converting a non-CCS ILP to an equivalent CCS ILP. But this step involves a non-trivial dregree of freedom. We will understand and use it, to translate between the equivalent ILPs in such a way, that the shortest path algorithm can be accelerated. 

\section{Converting natural CCS matricies to natural non-CCS matricies}
Our task is to convert CCS ILPs to non-CCS ILPs. But we need to do it in such a way, that their solution does not change. For that we can look at the underlying system of linear equations $A\vec x = \vec b$. It would be great, to convert any non-CCS $A$ to a CCS $A'$ and maybe also change $\vec b$ to some $\vec b'$ without changing the solution space, namely $\solspace(A;\vec b) = \solspace(A';\vec b')$. But as seen in lemma \ref{lemma:ilp_pre1}, the solution of CCS system of linear equations have a special property, namely that the component sum of all their solution is constant. This is not true in general. Consider the system of linear equations:
$$
\left(\begin{matrix}
    1 & 2\\
    1 & 2
\end{matrix}\right)
x = \left(\begin{matrix}
    2\\2
\end{matrix}\right)
$$
It yields among others the solutions $\vec x_1 = (2, 0)^\top$ and $\vec x_2 = (0, 1)^\top$. At the component sum of $\vec x_1$ is 2 while the component sum of $\vec x_2$ is 1, which is not equal. This means, that we have to accept, that we have to adapt the solution space slightly. In theorem \ref{theorem:column_sum_construction} we will achive that by adding another component to the solution, and thus adding a column to $A$. This first increases the degree of freedom and thus the number of solutions. To capture that increase of the size of the solution space space we will add another row to $A$, meaning another restricting equation. 

Before we dive into the construction, we need one defintion to capture matricies with no zero-columns. In our connext of natural numbers, this is equivalant to demanding that the columns all sum up to a strictly postive number. As this cncept will be useful later, beyond the scope of natural matracies, we will extend definition \ref{def:CCS} in that way.

\begin{definition}
    \label{def:PCS}
    Let $\mathbb{K}$ be an ordered field and $\alpha \in \mathbb{K}$. Let $S_{>\alpha}(\mathbb{K}^{m \times n})$ be the set of matricies of size $m \times n$, who's column all sum up to some number greater then $\alpha$, namely:
    $$S_{>\alpha}(\mathbb{K}^{m \times n}) := \{A \in \mathbb{K}^{m \times n}\mid \forall j \in [n]\colon \sum_{i=1}^{m} A_{ij} > \alpha\}$$
    For $\alpha = 0$, we will call these matricies \textbf{PCS matricies}, for \textbf{P}ositive \textbf{C}olumn \textbf{S}um.
\end{definition}

Thus natural PCS matricies are excactly those with constant column sum. Now we are ready to tackle the translation of an non-CCM ILP to a CCM ILP.

\begin{theorem}
    \label{theorem:column_sum_construction}
    Let $A\in S_{>0}(\N^{m \times n})$ and $\vec b \in \N^m$. Then, there exists $A' \in S_\alpha(\N^{(m+1) \times (n+1)}), \alpha > 0$ and $\vec b' \in \N^{m+1}$ such that for every $\vec x \in \N^n$:
    $$A\vec x = \vec b \Leftrightarrow \exists x_{n+1}\in \N\colon A' \smat{
        x_1\\
        \vdots\\
        x_n\\
        x_{n+1}
    } = \vec b'$$
\end{theorem}

\begin{proof}
    First we need to get an upper bound in the solutions $\vec x \in \N^n$ in $A\vec x=\vec b$. We will do that similarly as in the proof of Lemma \ref{lemma:ilp_pre1}. Let $s_j = \sum_{i=0}^{m} A_{ij}$, the column sum of the $j$-th column in $A$. Let $s := \min\{s_1, \dots, s_j\}$. Because $A$ has no zero-columns $s > 0$.
    $$\sum_{i=1}^m b_i = \sum_{i=1}^{m}\sum_{j=1}^{n}A_{ij} x_j = \sum_{j=1}^{n}\underbrace{\sum_{i=1}^{m}A_{ij}}_{s_j} x_j \geq s \cdot \sum_{j=1}^{n}x_j \Leftrightarrow \sum_{j=1}^{n}x_j \leq \frac{1}{s}\sum_{i=1}^{m}b_i$$
    Now we can construct $A'$ and $\vec b'$. Let $\alpha \geq \max\{s_1, \dots, s_n\}$ and $v_j := \alpha - s_j$. 
    $$A' :=
    \begin{pNiceArray}{cccc}[margin] 
    \Block[draw]{3-3}{A} & & & 0 \\
    & & & \vdots\\
    & & & 0\\
    v_1 & \dots  & v_n & \alpha 
    \end{pNiceArray} \in \N^{(m+1) \times (n+1)}
    \qquad \vec b' := \mat{b_1\\\vdots\\b_m\\\beta} \in \N^{m+1}$$
    It is clear that in $A'$, all columns sum up to $\alpha$. Observe also that $s \leq \alpha$. Because of Lemma \ref{lemma:ilp_pre1} (2) we need to set $\beta := k \cdot \alpha - \sum_{i=0}^{m}b_i$ for some $k \in \N$. We will set $k \geq \frac{1}{s}\sum_{i=1}^{m}b_i$. Thus we get $\beta \geq \frac{\alpha}{s}\sum_{i=1}^{m}b_i - \sum_{i=1}^{m}b_i \geq 0$, because $\frac{\alpha}{s} \geq 1$, which is needed, as $\beta \in \N$. Now we have to prove the equivalince:
    \begin{itemize}
        \item[``$\Leftarrow$''] Because the first $m$ rows in $A'\vec x=\vec b'$ are equialent to $A\vec x=\vec b$ discarding the last component of the solution $x_{n+1}$ we see that the vector $(x_1, \dots, x_n)^\top$ is indeed a solution to $A\vec x=\vec b$.
        \item[``$\Rightarrow$''] Let $(x_1, \dots, x_n)$ be the solution of $A\vec x=\vec b$. We we have to find a $x_{n+1} \in \N$ such that $\vec x' := (x_1, \dots, x_{n+1})$ is a solution to $A'\vec x' = \vec b'$. 
        
        Now we'll call $(x_1, \dots, x_{n+1}) =: \vec x'$. Because of Lemma \ref{lemma:ilp_pre1} (1) we need to set $x_{n+1} := k - \sum_{j=1}^{n}x_j$. Similarly to the discussion on $\beta$, we also have to make sure for $x_{n+1}$, that it is $\geq 0$. $x_{n+1} \geq k - \frac{1}{s}\sum_{i=1}^{m}b_i \geq \frac{1}{s}\sum_{i=1}^{m}b_i - \frac{1}{s}\sum_{i=1}^{m}b_i = 0$.
        
        Now, we need to check whether $A'\vec x'\stackrel{?}{=}\vec b'$. Because the first $m$ rows in $A'\vec x'=\vec b'$ are equialent to $A\vec x=\vec b$ discarding the last component of the solution $x_{n+1}$ we only have to check the last row. So we have to prove $(A'\vec x')_{n+1} = \beta$
        \begin{align*}
            (A'x')_{n+1} &= \sum_{j=1}^{n}v_jx_j + \alpha \cdot x_{n+1} = \sum_{j=1}^{n}(\alpha - s_j)x_j + \alpha \cdot x_{n+1} = \alpha \cdot \sum_{j=1}^{n}x_j - \sum_{j=1}^{n}s_jx_j + \alpha \cdot x_{n+1}\\
            &= \alpha \cdot \underbrace{\left(\sum_{j=1}^{n}x_j + x_{n+1}\right)}_k - \sum_{j=1}^{n}s_jx_j = \alpha \cdot k - \sum_{j=1}^{n}\sum_{i=1}^{m}A_{ij}x_j = \alpha\cdot k - \sum_{i=1}^{m}\underbrace{\sum_{j=1}^{n}A_{ij}x_j}_{b_i}\\
            &= \alpha\cdot k - \sum_{i=1}^{m}b_i = \beta
        \end{align*}
    \end{itemize}
\end{proof}
With this construction in our belts, we can take any PCS ILP and convert it into a CCS ILP and solve it using the shortest path frame work. But this conversion might not be great and we can improve it, which will result in smaller graphs. 

\section{Optimising the conversion}
\subsection{Introductary example}
In the last chapter, we saw that the runtine of solving ILPs in terms of a shortest path problem is heavily influenced by the number of layers in the corresponding graph. We called this number $k$ and it is equal to the sum of the components of the solutions of $A\vec x=\vec b$. We needed for $A$'s columns to all sum up to the same number. If $A$ did not have this property, we could construct a new matrix $A'$ that would have this property and alters the solution space minimally. For this, we used theorem \ref{theorem:column_sum_construction} where we were able to chose some $k$ which again drastically influences the runtime of the algorthm. There we had to put a restruction on $k$, namely
$$k \geq \frac{1}{\min_{j \in [n]} \left\{ \sum_{i=1}^{m}A_{ij}\right\}}\cdot \sum_{i=1}^{m}b_i$$
You may observe that equivalant linear systems of equations might produce different bounds. Let's look at the following simple example. We want to solve 
$$\mat{1&2\\0&6}\vec x = \mat{7\\18} \qquad \Rightarrow k \geq \frac{7 + 18}{\min\{1+0, 2+6\}} = 25$$
Meaning if we would solve an ILP with this system of linear equations we would need a graph with 25 layers. Now let's modify this system by multiplying the first row by 100:
$$\mat{100&200\\0&6}\vec x = \mat{700\\18} \qquad \Rightarrow k \geq \frac{700 + 18}{\min\{100+0, 200+6\}} = \frac{718}{100} \Rightarrow k \geq 8$$
We have achieved a major reduction of the number of layers and thus dramatically shrank the graph. In the following arguments we explore these kinds of manipulations and discover a strategy on how you would systimatically minimizing $k$. But first, we have to undestand the behavior better.

\subsection{Getting a grip: Define a cost function}
Lets first define a few helpful functions. Let $k\colon S_{>0}(\N^{m \times n}) \times \N^m \to \R$ be
$$k(A, \vec b) := \frac{1}{\min_{j \in [n]} \left\{ \sum_{i=1}^{m}A_{ij}\right\}}\cdot \sum_{i=1}^{m}b_i$$
So we need to find a $k \geq k(A, \vec b)$. As $k(A, \vec b)$ dictates how many layers the resulting graph will have, we can think of it as some kind of a cost function. Now we will generalize this cost function. But how are we going to go about that? The goal is to do very sepcific Gau√ü manipulation steps to $A\vec x = \vec b$. But not all values of the resultung system of equations are interesting for $k$ but just the column sums in $A$ and the component sum in $\vec b$. To figure this out, the only thing we need to keep track is, by how much each row has been scaled up or down, in other words how much in contributes to the resulting system of equation. To track that, we will save these values in some vector $\vec\mu$, where $\mu_i$ dictates by how much the $i$-th row has been scaled up. $\opvec(1)$, the vector only consisting of ones, thus represents the orignal matrix, as each row has not been scaled ($\Leftrightarrow$ scaled by 1). Now figuring out the column sum in the $j$-th column after the transformation boils down to the standard dot-product of the $j$-th column $\vec a_j$ and $\vec\mu$. For example with $\vec \mu = \opvec(1)$, $\sprod{\vec a_j}{\opvec(1)}$ is really just the $j$-th column sum in the original $A$. 
$$k(A, \vec b; \vec \mu) := \frac{\sprod{\vec{\mu}}{\vec b}}{\min_{j \in [m]} \sprod{\vec \mu}{\vec a_j}}$$
where $\vec a_j$ is the $j$-th column of $A$. Because we want teh columns sum to always stay positive after the transformation we want to restrict $\vec\mu$ to those values where for all column $\vec a_j$ hold that $\sprod{\vec a_j}{\vec\mu} > 0$. More formally: we restruct $\vec\mu$ to:
$$\vec \mu \in U_A := \{\vec x \in \R^m \mid \forall j \in [m]\colon \sprod{\vec x}{\vec a_j} > 0\}$$
Note that because all entries of $A$ are non-negative, $U_A$ is a superset of $\R_{>0}^m$. We also see that $k(A, \vec b) = k(A, \vec b; \opvec(1))$, where $\opvec(1) = (1, \dots, 1)^\top$. And also $\opvec(1) \in U_A$, because $\opvec(1) \in \R^m_{>0}$. It is left to show, why exactly this generalization is as useful as promised. This will be handled by the next lemma:
\begin{lemma}
    Let $A \in \N^{m \times n}, \vec b \in \N^m$ such that $A$ has no zero columns. Then the following to statements will hold:
    \begin{enumerate}
        \item[1)] $\forall \vec \mu \in U_A \cap \Q^m$ exist $A' \in \N^{m \times n}, \vec b' \in \N^m$, such that $\solspace(A, \vec b) = \solspace(A', \vec b')$ and
        $k(A', \vec b') = k(A, \vec b; \vec\mu)$
        \item[2)] $\forall  A' \in \N^{m \times n}, \vec b' \in \N^m$, such that $\solspace(A, \vec b) = \solspace(A', \vec b')$ exists a $\vec \mu \in U_A \cap \Q^m$, such that 
        $k(A', \vec b') = k(A, \vec b; \vec\mu)$.
    \end{enumerate}
\end{lemma}
\begin{proof}
    \begin{enumerate}
        \item[1)] Because $k(A, \vec b; \vec \mu) = k(A, \vec b; \lambda \vec \mu)$ for $\lambda > 0$ we can w.l.o.g. assume that $\vec \mu \in \Z^m$. Because we need to construct $A'$ and $\vec b'$ such that $\solspace(A, \vec b) = \solspace(A', \vec b')$ there must exist a $C \in \GL_m(\Q)$ such that $A' = C\cdot A$ and $\vec b = C\cdot \vec b$. Thus it suffices to construct $C$. So the task will be to select $C$ such that
        $$C^\top \cdot \opvec(1) = \lambda \vec\mu, \quad A' = C\cdot A \in \N^{m \times n} \quad \textrm{and} \quad \det(C) \neq 0$$
        for some $\lambda \in \N \setminus \{0\}$. Then it will hold that:
        \begin{align*}
            k(A', \vec b') &= k(A', \vec b'; \opvec(1)) = k(C \cdot A, C \cdot \vec b; \opvec(1))\\
            &= \frac{\sprod{\opvec(1)}{C \cdot \vec b}}{\min_{j \in [m]} \sprod{\opvec(1)}{C \cdot \vec a_j}} = \frac{\sprod{C^\top \cdot \opvec(1)}{\vec b}}{\min_{j \in [m]} \sprod{C^\top \cdot \opvec(1)}{\vec a_j}}\\
            &= \frac{\sprod{\lambda\vec{\mu}}{\vec b}}{\min_{j \in [m]} \sprod{\lambda\vec \mu}{\vec a_j}} = k(A, \vec b; \lambda\vec\mu) \stackrel{\checkmark}{=} k(A, \vec b; \vec\mu)
        \end{align*}
        How do we actaully find that $C$. We will split that task into 2 tasks, by constrcuting 2 matricies $C', D  \in \GL_m(\Z)$, setting $C := C' \cdot D$ with the properties:
        \begin{itemize}
            \item $C'^\top \cdot \opvec(1) = \lambda \cdot \opvec(1)$
            \item $D^\top \cdot \opvec(1) = \vec\mu$
            \item $A' = C' \cdot D\cdot A$ has only non-negative entries
        \end{itemize}
        In other words: $D$ will make sure that each row does actually contribute as much as specified in $\vec\mu$ to the transformed system of linear equations and $C'$ makes sure that all entries in $A'$ are positive.

        If we can find such $C', D$ we are done, because $\det(C) \neq 0$ and $ A' = C\cdot A \in \N^{m \times n}$ is clear and $C^\top \cdot \opvec(1) = D^\top \cdot C'^\top\cdot \opvec(1) = \lambda \cdot D^\top \opvec(1) \stackrel{\checkmark}{=}\lambda\vec\mu$. So let's do it one by one:

        $$D := \mat{
            d_1 & e_2 & 0 & \dots & 0 & 0\\
            0 & d_2 & e_3 & \dots & 0 & 0\\
            0 & 0 & d_3 & \dots & 0 & 0\\
            \vdots & \vdots & \vdots & \ddots & \vdots & \vdots\\
            0 & 0 & 0 & \dots & d_{m-1} & e_m\\
            e_1 & 0 & 0 & \dots & 0 & d_m
        } \quad\textrm{with}\quad 
        \begin{matrix}
            d_i := \begin{cases}
                \mu_i &\textrm{if}\quad \mu_i \neq 0\\
                1 &\textrm{if}\quad \mu_i = 0
            \end{cases}\\\\
            e_i := \begin{cases}
                0 &\textrm{if}\quad \mu_i \neq 0\\
                -1 &\textrm{if}\quad \mu_i = 0
            \end{cases}
        \end{matrix}$$
        we see that $(D^\top \cdot \opvec(1))_i = d_i + e_i \stackrel{\checkmark}{=} \mu_i$. We still have to make sure that $D$ is invertable, meaning $\det(D) \neq 0$. Let's compute the determinante by applying laplace expansion to the first column. We get:
        \begin{align*}  
            \det(D) &= d_1 \cdot \det\mat{
                d_2 & e_3 & \dots & 0 & 0\\
                0 & d_3 & \dots & 0 & 0\\
                \vdots & \vdots & \ddots & \vdots& \vdots\\
                0 & 0 & \dots & d_{m-1} & e_m\\
                0 & 0 & \dots & 0 & d_m
            } \pm e_1 \cdot \det\mat{
                e_2 & 0 & \dots & 0 & 0\\
                d_2 & e_3 & \dots & 0 & 0\\
                \vdots & \vdots & \ddots & \vdots& \vdots\\
                0 & 0 & \dots & e_{m-1} & 0\\
                0 & 0 & \dots & d_{m-1} & e_m
            }\\
            &= d_1 \cdot d_2 \cdot d_3 \cdot ... \cdot d_m \pm e_1 \cdot e_2 \cdot e_3 \cdot ... \cdot e_m 
        \end{align*}
        Because $\vec\mu \neq \vec0$, we know that at least one $e_i$ must be 0. Thus $e_1 \cdot e_2 \cdot e_3 \cdot ... \cdot e_m = 0 \Rightarrow \det(D) = d_1 \cdot d_2 \cdot d_3 \cdot ... \cdot d_m \neq 0$.

        Now we construct $C'$. For that let $\tilde A := D \cdot A$ and $c := \max_{i \in [m], j\in[n]} |\tilde A_{ij}| > 0$ the largest absolute value in $\tilde A$. Let $\opmat_m(c) \in \N^{m\times m}$ be the matrix, with only $c$ as entries. Then we can chose $C'$:
        $$C' := \imat_m + \opmat_m(c)$$
        It is easy to see, that $C'^\top \opvec(1) = \imat_m\cdot \opvec(1) + \opmat_m(c)\cdot \opvec(1) = \opvec(1) + mc \opvec(1) = (1 + mc)\opvec(1)$. So the necessary property holds, with $\lambda = mc+1$. We have to check again, that $C'$ is indeed invertable. Let's assume there exists a vector $\vec v \in \Q^m, \vec v \neq \vec0$ such that $C'\vec v = \vec 0 \Leftrightarrow \vec0 = \vec v + \opmat_m(c)\vec v \Leftrightarrow \opmat_m(c)\vec v = -\vec v$. For that to be true, $\opmat_m(c)$ must have the eigenvalue $-1$. It is easy to guess all $m$ eigenvalues of $\opmat_m(c)$: eigenvector $\opvec(1)$ yields the eigenvalue $cm>0$ and the family of vectors in the shape $(1, 0, \dots, 0, -1, 0, \dots, 0)$ yield $m-1$ eigenvalues of value $0$. Thus $-1$ is not an eigenvalue of $\opmat_m(c)$ and $\vec v$ does not exist and $C'$ is invertable.

        The last step is making sure, that $A'$ only contains non-negative entries. Remember that $\tilde A = D \cdot A$, thus $A' = C' \cdot \tilde A$. We see that $(\tilde A^\top \opvec(1))_j =(A^\top\cdot D^\top\cdot \opvec(1))_j = (A^\top \vec\mu)_j = \sprod{\vec a_j}{\vec\mu} > 0$. But because all numbers involved are whole numbers, we even further now that $(\tilde A^\top \opvec(1))_j \geq 1$. Now let's check the non-negativity of $A'$:

        \begin{align*}
            A'_{ij} &= (C'\cdot \tilde A)_{ij} = (\imat_m \cdot \tilde A)_{ij} + (\opmat_m(c)\cdot \tilde A)_{ij}\\
            &= \tilde A_{ij} + \sum_{k=1}^{m}\opmat_m(c)_{ik} \tilde A_{kj} = \tilde A_{ij} + c \cdot \sum_{k=1}^{m} \tilde A_{kj} = \tilde A_{ij} + c \cdot (\tilde A^\top \opvec(1))_j\\
            &\geq \tilde A_{ij} + c \geq - |\tilde A_{ij}| + c \geq - c + c = 0
        \end{align*}
        
        \item[2)] Because $\solspace(A, \vec b) = \solspace(A', \vec b')$, there mus exist $C \in \GL_m(\Q)$ such that $A' = C \cdot A \in \N^{m \times n}$ and $\vec b' = C \cdot \vec b \in \N^m$. As in 1) we will chose $\vec \mu := C^\top \cdot \opvec(1)$ and thus:
        \begin{align*}
            k(A, \vec b; \vec\mu) &= \frac{\sprod{\vec\mu}{\vec b}}{\min_{j\in [m]}\sprod{\vec\mu}{\vec a_j}} = \frac{\sprod{C^\top\cdot\opvec(1)}{\vec b}}{\min_{j\in [m]}\sprod{C^\top\cdot\opvec(1)}{\vec a_j}}\\
            &= \frac{\sprod{\opvec(1)}{C\cdot\vec b}}{\min_{j\in [m]}\sprod{\opvec(1)}{C\cdot\vec a_j}} = k(C\cdot A, C\cdot\vec b; \opvec(1)) \stackrel{\checkmark}{=} k(A', \vec b')
        \end{align*}
        It is left to check whether $\vec \mu \in U_A$. Because $A$ had no zero columns, $A'$ also has none, which means that for every $j \in [m]$:
        $$0 < \sprod{\opvec(1)}{\vec a'_j} = \sprod{\opvec(1)}{C \cdot \vec a_j} = \sprod{C^\top \cdot \opvec(1)}{\vec a_j} = \sprod{\vec \mu}{\vec a_j} \qquad \Rightarrow\vec \mu \in U_A$$

    \end{enumerate}
\end{proof}

Statement 2) says that for any other equialent system of equations we find a $\vec \mu$ that represents its $k$-function. And statement 1) says that for every $\vec \mu$ we find a corresponding system of equations. This means that all equivalent systems of equation are exactly covered by the function $k(A, \vec b; \vec\mu)$ for some $\vec \mu$. This means, for finding the optimal $k$ we just need to minimize $k(A, \vec b; \vec \mu)$ over $\vec \mu$.

Before we proceed, one last note about continuity. Because $k(A, \vec b; \vec \mu)$ is continous when changing $\vec\mu$, we will find for every $\varepsilon > 0$ and $\vec\mu \in U_A$ a $\vec\mu' \in U_A \cap \Q^m$ such that $|k(A, \vec b; \vec \mu) - k(A, \vec b; \vec \mu')| < \varepsilon$. This means all further discussions can be done in $\R^m$.

Let's simplify notation. We will now consider $A \in \N^{m \times n}$ and $\vec b \in \N^m$ as given. And we will write $k(\vec \mu) := k(A, \vec b; \vec \mu)$ and also $U := U_A$. Our goal is to minimize $k(\vec \mu)$ over $U$.

By 2 simple observation we can allready make a lot of progress. Note that $\delta U = \{\vec x \in \R^m \mid \min_{j \in [n]}\sprod{\vec x}{\vec a_j} = 0\}$ and thus $k(\vec \mu)$ blows up to infinity when $\vec \mu \to \delta U$. Thus the minimum we are seeking cannot lie on the border of $U$. Furthermore note that $k(\vec \mu) = k(\lambda \vec\mu)$ for any $\lambda > 0$. This means that $k(\vec \mu)$ will have the same value along a ray starting at the origin. Thus it is suffictiant to only consider a $m-1$-dimensional surface of a sphere, which is compact. Remeber that any continous function on will attain its maximum and minimum at least once on a compect set. Thus we know, that any minima that will be reached are local minima in $U$.

Because we now know that we are only interested in local minima in $U$, we can start analyzing $k(\vec \mu)$ a little bit closer. But first, a little bit more notation:
$$U_j := \{\vec x \in U \mid \forall j' \in [n]\colon \sprod{\vec x}{\vec a_j} \leq \sprod{\vec x}{\vec a_{j'}}\} \subseteq U$$
$U_j$ is thus the area where $\sprod{\vec\mu}{\vec a_j}$ is the smallest dot product and hence:
$$\restrict{k(\vec\mu)}{U_j} = \frac{\sprod{\vec\mu}{\vec b}}{\sprod{\vec\mu}{\vec a_j}}$$
Especially interesting are those areas, where different $H_j$'s meet. This can be seen in the next lemma:

\begin{lemma}
    \label{lemma:k_behavior}
    Let $\vec a_1, \dots, \vec a_n \in \N^m$, $U_j := \{\vec x \in \R^m \mid \forall j' \in [n]\colon \sprod{\vec x}{\vec a_j} \leq \sprod{\vec x}{\vec a_{j'}}\}$ and $k(\vec \mu) := \frac{\sprod{\vec{\mu}}{\vec b}}{\min_{j \in [m]} \sprod{\vec \mu}{\vec a_j}}$. Let $j_1, \dots, j_r \in [n]$ a selection of indices, such that $U_{j_1} \cap \dots \cap U_{j_r} \neq \emptyset$.  Then it will hold that:
    \begin{itemize}
        \item[1)] $\vec b \in \Span(\vec a_{j_1}, \dots, \vec a_{j_r}) \Rightarrow \restrict{k(\vec \mu)}{U_{j_1} \cap \dots \cap U_{j_r}}$ is constant.
        \item[2)] $\vec b \notin \Span(\vec a_{j_1}, \dots, \vec a_{j_r}) \Rightarrow \restrict{k(\vec \mu)}{U_{j_1} \cap \dots \cap U_{j_r}}$ is strongly monotone.
    \end{itemize}
\end{lemma}
\begin{proof}
    Because $\vec \mu \in U_{j_1}\cap \dots\cap U_{j_r}$ the following 2 statements will hold:
    $$\sprod{\vec{\mu}}{\vec a_{j_1}} = \dots = \sprod{\vec{\mu}}{\vec a_{j_r}} \quad\textrm{and}\quad k(\vec\mu) = \frac{\sprod{\vec \mu}{\vec b}}{\sprod{\vec \mu}{\vec a_{j_1}}}$$
    Now let's consider the two cases seperately\\
    \textbf{Case 1} $\vec b \in \Span(\vec a_{j_1}, \dots, \vec a_{j_r}) \Rightarrow\exists c_1, \dots, c_r \in \R\colon \vec b = c_1\vec a_{j_1} + \dots + c_1\vec a_{j_r}$.
    $$k(\vec\mu) = \frac{\sprod{\vec \mu}{\vec b}}{\sprod{\vec \mu}{\vec a_{j_1}}} = c_1 \cdot \underbrace{\frac{\sprod{\vec \mu}{\vec a_{j_1}}}{\sprod{\vec \mu}{\vec a_{j_1}}}}_1 + \dots + c_r \cdot \underbrace{\frac{\sprod{\vec \mu}{\vec a_{j_r}}}{\sprod{\vec \mu}{\vec a_{j_1}}}}_1 = c_1 + \dots + c_r \quad \Rightarrow k(\vec\mu)\textrm{ is constant}$$
    \textbf{Case 2} $\vec b \notin \Span(\vec a_{j_1}, \dots, \vec a_{j_r}) \Rightarrow\lnot\exists c_1, \dots, c_r \in \R\colon \vec b = c_1\vec a_{j_1} + \dots + c_1\vec a_{j_r}$. We want to optimize a function subject to some constraints. For that we will be using the method of Lagrange multiplieres. We have $r-1$ constraints of the following shape:
    $$\sprod{\vec\mu}{\vec a_{j_l}} = \sprod{\vec\mu}{\vec a_{j_{l+1}}} \Leftrightarrow \sprod{\vec\mu}{\vec a_{j_l} - \vec a_{j_{l+1}}} = 0, \quad l \in [r-1]$$
    Thus we get the Lagrangian
    \begin{align*}
        \lagrangian(\vec\mu, \lambda_1, \dots, \lambda_{r-1}) &= \frac{\sprod{\vec \mu}{\vec b}}{\sprod{\vec \mu}{\vec a_{j_1}}} - \sum_{l=1}^{r-1}\lambda_l \sprod{\vec\mu}{\vec a_{j_l} - \vec a_{j_{l+1}}}\\
        \vec0 \stackrel{!}{=}\nabla_{\vec \mu} \lagrangian(\vec\mu, \lambda_1, \dots, \lambda_{r-1}) &= \frac{\vec b\sprod{\vec\mu}{\vec a_{j_{1}}} - \sprod{\vec\mu}{\vec b}\vec a_{j_{1}}}{\sprod{\vec\mu}{\vec a_{j_{1}}}^2} - \sum_{l=1}^{r-1}\lambda_l(\vec a_{j_l} - \vec a_{j_{l+1}}) \\
        \Rightarrow\quad \vec0 &= \vec b\sprod{\vec\mu}{\vec a_{j_{1}}} - \sprod{\vec\mu}{\vec b}\vec a_{j_{1}} - \sprod{\vec\mu}{\vec a_{j_{1}}}^2\cdot\sum_{l=1}^{r-1}\lambda_l(\vec a_{j_l} - \vec a_{j_{l+1}})\\
        \Rightarrow\quad \vec b &= \frac{\sprod{\vec \mu}{\vec b}}{\sprod{\vec \mu}{\vec a_{j_{1}}}}\vec a_{j_{1}} + \sum_{l=1}^{r-1} \lambda_r \sprod{\vec\mu}{\vec a_{j_{1}}}(\vec a_{j_l} - \vec a_{j_{l+1}})
    \end{align*}
    Which is just a linear combination of $\vec a_{j_1}, \dots \vec a_{j_r}$, which we forbid in the first place. Thus this also yields a contradiction meaning that $k(\vec\mu)$ is strongly monotone.
\end{proof}

A local minimum can only be attained when the gradient vanishes. Because of the last lemma, I only need to find different sets of $\{\vec a_{j_1}, \dots, \vec a_{j_r}\} \subseteq \{\vec a_1, \dots, \vec a_n\}$ such that $\vec b \in \Span(\vec a_{j_1}, \dots, \vec a_{j_r})$ and $U_{j_1} \cap \dots \cap U_{j_r} \neq \emptyset$ and then compare their $k(\vec\mu)$ for some $\vec\mu \in U_{j_1} \cap \dots \cap U_{j_r}$. The right selection of $\{\vec a_{j_1}, \dots, \vec a_{j_r}\}$ will be key in understanding the minimisation of $k(\vec\mu)$. 

\subsection{Minimzing the cost function}
Now we need to find a $\vec\mu$ that minizes $k(\vec\mu)$. I will now display a rough draft on how to find the correct $\mu$ discarding the details. But I will be enough to be able to show the correctness of this approach. Afterwards, I will fill in the details on how to efficiently do each step. But first a definition:
\begin{definition}
    Let $M \in \N^{m\times n}$ be a matrix. Then $\Delta(M) \in \Z^{(n-1)\times m}$ is also a matrix such that 
    $$\Delta(M)_{i,j} = M_{j,i} - M_{j,i+1}$$
\end{definition}
This is useful, as all $\vec x$ solving $\Delta(M)\vec x = \vec 0$ will have the property that all columns of $M$ (let they be $\vec m_1, \dots, \vec m_n$) yield the same dot-product with $\vec x$, in other words $\sprod{\vec x}{\vec m_1} = \dots = \sprod{\vec x}{\vec m_n}$.

Now let's discribe the draft for the algorithm. It mainly involves two steps:
\begin{algorithm}
    \label{algo}
    \begin{enumerate}
        \item Compute the convex hull of $\vec a_1, \dots, \vec a_n$. Send a ray from the origin to $\vec b$ and record the first facet of the convex hull it collides with. If it hits a lower dimensional facet, we'll take that. Let the vertecies of that facet be $\vec a_{j_1}, \dots, \vec a_{j_r}$. Create a new matrix $A' := (\vec a_{j_1}, \dots, \vec a_{j_r}) \in \N^{m \times r}$.
        \item Find a $\vec\mu \in \solspace(\Delta(A'), \vec0) \setminus \solspace(A'^\top, \vec0)$. This $\vec\mu$ will yield the minimal value for $k(\vec\mu)$.
    \end{enumerate}
\end{algorithm}

Now we have to answer the question of correctness. And we will do that in a few steps. First we will show, that the algorithm always output some value, than that this value is a local minimum and finally that this local minimum is the smallest local minimum there is and thus the global minimum. But before that, I will define some notions, that we will need throught our jurney.
\begin{definition}
    \label{def:algo_basic}
    Let $A \in \N^{m \times n}$ be a matrix, such that $A$ does not contain a zero column. Let $\vec a_j$ be the $j$-th column of $A$. Let $B := [\vec a_{j_1}, \dots, \vec a_{j_r}]$ and $A' \in \N^{m\times r}$ be as in step 2 in the algorithm \ref{algo}. Let $\vec b \in \N^m$ such that $\exists \vec x \in \N^n\colon A\vec x = \vec b$. Let $U := \{\vec\mu \in \R^m\mid \forall j \in [n]\colon\sprod{\vec\mu}{\vec a_j} > 0\}$. Let $U_j := \{\vec\mu \in U \mid \forall j'\in[n]\colon \sprod{\vec\mu}{\vec a_j} \leq \sprod{\vec\mu}{\vec a_{j'}}\}$. Let $k\colon \N^m\to\R$ be defined as 
    $$k(\vec\mu) := \frac{\sprod{\vec\mu}{\vec b}}{\min_{j\in[n]}\sprod{\vec\mu}{\vec a_j}}$$
\end{definition}

\subsubsection{Existance of a result}
First we need to ask ourselves, whether the algorithm always yields an output. Step 1 requires that the ray actually intersects the convex hull while for step 2 $\solspace(\Delta(A'), \vec0) \setminus \solspace(A'^\top, \vec0)$ must be non-empty. This I will clarify now:

\begin{lemma}
    \label{lemma:b_behind_hull}
    Let $\vec b, \vec x, \vec a_1, \dots, \vec a_n$ as in definition \ref{def:algo_basic}. Then there exists $\gamma \in \Q$, $0 < \gamma \leq 1$ such that $\gamma \vec b$ lies inside the convex hull of $\vec a_1, \dots, \vec a_n$.
\end{lemma}
\begin{proof}
    By assumption, we know that $\vec b = \sum_{j=1}^{n}x_j \vec a_j$ with $\sum_{j=1}^{n}x_j =: s \geq 1$. Let $\gamma := \frac{1}{s}$, thus $0 < \gamma \leq 1$. Now let's construct $\gamma\vec b$ using $\vec a_1, \dots, \vec a_n$:
    $$\gamma\vec b = \frac{1}{s}\sum_{j=1}^{n}x_j \vec a_j = \sum_{j=1}^{n}\frac{x_j}{s}\vec a_j =: \sum_{j=1}^{n} \lambda_j\vec a_j \qquad \mathrm{with}\quad \sum_{j=1}^{n}\lambda_j = \frac{1}{s}\cdot \sum_{j=1}^{n} x_j = \frac{1}{s}\cdot s = 1, \lambda_j \geq 0$$
    Thus $\gamma\vec b$ is representable as a convex combination of $\vec a_1, \dots, \vec a_n$ and therefore lies in their convex hull.
\end{proof}
This lemma ensures that on the path from the origin to $\vec b$ we will hit the convex hull, which ensures that step 1 will always have a result. We will handle the non-emptyness of $\solspace(\Delta(A'), \vec0) \setminus \solspace(A'^\top, \vec0)$ in two seperate lemmas: 

\begin{lemma}
    \label{lemma:vertecies_are_basis}
    Let $\vec a_{j_1}, \dots, \vec a_{j_r}$ from definition \ref{def:algo_basic} are lineally independent.
\end{lemma}
\begin{proof}
    I'll show that $\vec 0 \notin \operatorname{aff}(\vec a_{j_1}, \dots, \vec a_{j_r})$ by contradiction which then implies that $\vec a_{j_1}, \dots, \vec a_{j_r}$ are lineally independent, because if not, there exists a linear dependence $\vec 0 = \lambda_1\vec a_{j_1} + \dots + \lambda_r\vec a_{j_r}$ with $\lambda_1 + \dots + \lambda_r = 1 \Rightarrow\exists\lambda_j\colon\lambda_j \neq 0$. 
    
    Let's assume that $\vec 0 \in \operatorname{aff}(\vec a_{j_1}, \dots, \vec a_{j_r})$, thus $\vec 0 = \lambda^{\vec 0}_1 \vec a_{j_1} + \dots + \lambda^{\vec 0}_r \vec a_{j_r}$ with $\lambda^{\vec 0}_1 + \dots + \lambda^{\vec 0}_r = 1$. Let $\vec p$ the first intersection of the ray to $\vec b$ and the convex hull thus $\vec p = \lambda^{\vec p}_1 \vec a_{j_1} + \dots + \lambda^{\vec p}_r \vec a_{j_r}$ with $\lambda^{\vec p}_1 + \dots + \lambda^{\vec p}_r = 1$ and $\lambda^{\vec p}_j \geq 0$ Let $\nu \in [0, 1]$. Any point $\nu\vec p$ is also representable as an affine combination, because
    \begin{align*}
        \nu \vec p &= \nu \vec p + (1-\nu)\vec 0 = \nu\lambda^{\vec p}_1 \vec a_{j_1} + \dots + \nu\lambda^{\vec p}_r \vec a_{j_r} + (1-\nu)\lambda^{\vec 0}_1 \vec a_{j_1} + \dots + (1-\nu)\lambda^{\vec 0}_r \vec a_{j_r}\\
        &=(\nu\lambda^{\vec p}_1 + (1-\nu)\lambda^{\vec 0}_1)\vec a_{j_1} + \dots + (\nu\lambda^{\vec p}_r + (1-\nu)\lambda^{\vec 0}_r)\vec a_{j_r} =: \lambda_1(\nu)\vec a_{j_1} + \dots + \lambda_r(\nu)\vec a_{j_r}
    \end{align*}
    It is easy to see, that indeed $\forall \nu\in[0, 1]: \lambda_1(\nu) + \dots + \lambda_r(\nu) = 1$. $\lambda_j(\nu)$ are linear functions, with $\lambda_j(0) = \lambda_j^{\vec 0}$ and $\lambda_j(1) = \lambda_j^{\vec p}$. Because all $\vec a_j$ have non-negative entries and none of them is $\vec 0$ we know that there must be at least one $\lambda_j^{\vec 0}$ that is negative. Because all $\lambda_j^{\vec p}$ are non-negative, there must exist at least one function $\lambda_j(\nu)$ that has a zero in $\nu \in [0, 1]$. Let's select the highest zero $\nu_0 := \max\{\nu \in [0, 1]\mid \exists j\in [n]\colon \lambda_j(\nu) = 0\}$. Observe that, if $\nu \geq \nu_0$ that $\nu\vec p \in \operatorname{conv}(\vec a_{j_1}, \dots, \vec a_{j_r})$. But $\vec p$ is the first intersection with the convex hull and this forces $\nu_0 = 1$. Well, that means that at least one coefficiant in $\lambda^{\vec p}_1, \dots, \lambda^{\vec p}_r$ is zero. Thus there exists a lower dimensional facet, namely the convex hull of the corners who's coefficiants are non-zero. Thus $\vec a_{j_1}, \dots, \vec a_{j_r}$ couldn't have been selected by the algorithm. Tadaaa the contradiction.
\end{proof}

\begin{lemma}
    Let $A'$ be as in definition \ref{def:algo_basic}. Then $\dim(\solspace(\Delta(A'), \vec0)) > \dim(\solspace(A'^\top, \vec0))$.
\end{lemma}
\begin{proof}
    Observe that $\dim(\solspace(\Delta(A'), \vec0)) = \nullity(\Delta(A'))$ and $\dim(\solspace(A'^\top, \vec0)) = \nullity(A'^\top) = \nullity(A')$. Because of lemma \ref{lemma:vertecies_are_basis} it will hold that $\rank(A') = r$. As $\Delta(A')$ has only $r-1$ rows, $\rank(\Delta(A')) \leq r-1 < r$. Now we can apply the Rank-nullity theorem to $A'^\top$ and $\Delta(A')$.
    $$m = \rank(A'^\top) + \nullity(A'^\top) \Leftrightarrow \nullity(A^\top) = m - r$$
    $$m = \rank(\Delta(A')) + \nullity(\Delta(A')) < r + \nullity(\Delta(A'))\Leftrightarrow \nullity(\Delta(A')) > m - r$$
    Combining these two results yields $\nullity(\Delta(A')) > \nullity(A'^\top)$ the result we wanted to show.
\end{proof}
Subtracting a lower dimensional vector space from a bigger dimensional one will always yield some vectors and thus $\solspace(\Delta(A'), \vec0) \setminus \solspace(A'^\top, \vec0)$ is non-empty.

\subsubsection{The result is a local minimum}
We already saw in lemma \ref{lemma:k_behavior} that $\restrict{k(\vec\mu)}{U_{j_1} \cap \dots \cap U_{j_r}}$ is constant if and only if $\vec b \in \Span(\vec a_{j_1}, \dots, \vec a_{j_r})$. Therfore, the selected $\vec\mu$ must be in some non-empty $U_{j_1} \cap \dots \cap U_{j_r}$. Because $\vec\mu \in \solspace(\Delta(A'), \vec0)$, we know that $\sprod{\vec\mu}{\vec a_{j_1}} = \dots = \sprod{\vec\mu}{\vec a_{j_r}} =: d$. Wlog $d \geq 0$. Because $\vec\mu \notin \solspace(A'^\top, \vec0)$, we see that $d \neq 0$ and thus $d > 0$. This seams fine, there still might be a problem. If we find some $\vec a_j$ such that $\sprod{\vec\mu}{\vec a_j} < d$, then $\vec\mu \notin U_{j_1} \cap \dots \cap U_{j_r}$, the set might even be empty. That this doesn't happen, is clear when we look at the follwing two lemmas:

\begin{lemma}
    \label{lemma:affine_space}
    Let $\vec v_1,\dots,\vec v_r$ be a linearly independent set in $\R^n$, and let $H$ be the unique $(r-1)$-dimensional affine space passing through $\vec v_1,\dots,\vec v_r$. Then $H$ separates the $r$-dimensional subspace span$(\vec v_1,\dots,\vec v_r)$ into two open half-spaces: one piece $S_0$ contains the origin and the other piece $S_1$ does not. Furthermore and more crucially: if $\vec p = \sum_{i=1}^r \lambda_i \vec v_i$ is any vector in span$(\vec v_1,\dots,\vec v_r)$, then
    $$
    \sum_{i=1}^r \lambda_i < 1 \text{ if and only if } \vec p \in S_0.
    $$
\end{lemma}
\begin{proof}
    Let's first show that $\vec 0$ is not in $H$. If it were, we would find it's barycentric coordinates $\lambda_1, \dots, \lambda_r$:
    $$\vec 0 = \sum_{i=1}^r \lambda_i \cdot \vec v_i \qquad\mathrm{with}\quad\sum_{i=1}^{r}\lambda_i = 1$$
    Because the sum of the coordinates is 1, we know that at least one $\lambda_i$ is non-zero. That means we found a lineaer dependence which contradicts the fact that $\vec v_1,\dots,\vec v_r$ are linearly independent, and thus $\vec 0$ cannot be in $H$.

    Now let $\vec p \neq \vec 0$ be arbetrairy. We will consider three cases:
    \begin{itemize}
        \item[\textbf{Case 1}] The ray $\{\gamma\vec p\mid \gamma \in \R\}$ is parallel to $H$. Because this ray has never passed $H$ and $\vec 0 \in S_0$, we know that $\vec p \in S_0$. Adding any vector in $H$ (e.g. $\vec v_1$) to $\vec p$ will thus create a point in $H$, which again will have the barycentric coordinates $\lambda_1, \dots, \lambda_r$. Now:
        $$\vec p + \vec v_1 = \sum_{i=1}^{r}\lambda_i\cdot \vec v_i \Leftrightarrow \vec p = \sum_{i=1}^{r}\lambda_i\cdot \vec v_i - \vec v_1\qquad\mathrm{with}\quad\sum_{i=1}^{r}\lambda_i = 1$$
        Thus the coordinates of $\vec p$ in terms of $\vec v_1, \dots, \vec v_r$ sum up to $\sum_{i=1}^{r}\lambda_i - 1 = 1 - 1 = 0 \stackrel{\checkmark}{<} 1$.

        \item[\textbf{Case 2}] The ray $\{\gamma\vec p\mid \gamma \in \R\}$ is not parallel to $H$ and $\vec p \in S_0$. That means that there exists a point $\vec p' \in H$, where the the ray hits $H$. Let $\vec p' := \gamma \vec p$. As $\vec p \in S_0$, we know that $\gamma > 1$. As $\vec p' \in H$ it will have the $\lambda_1, \dots, \lambda_r$. Now:
        $$\vec p = \frac{1}{\gamma}\vec p' = \frac{1}{\gamma} \sum_{i=1}^{r}\lambda_i\cdot \vec v_i \qquad\mathrm{with}\quad\sum_{i=1}^{r}\lambda_i = 1$$
        Thus the coordinates of $\vec p$ in terms of $\vec v_1, \dots, \vec v_r$ sum up to $\frac{1}{\gamma}\sum_{i=1}^{r}\lambda_i = \frac{1}{\gamma}\stackrel{\checkmark}{<} 1$.

        \item[\textbf{Case 3}] The ray $\{\gamma\vec p\mid \gamma \in \R\}$ is not parallel to $H$ and $\vec p \in S_1$. That means that there exists a point $\vec p' \in H$, where the the ray hits $H$. Let $\vec p' := \gamma \vec p$. As $\vec p \in S_1$, we know that $\gamma \leq 1$. By the exact same argument as in case 2, we comclude that the coordinates of $\vec p$ in terms of $\vec v_1, \dots, \vec v_r$ sum up to a number greater than 1.
    \end{itemize}
\end{proof}

\begin{lemma}
    Let $\vec a_1, \dots, \vec a_n$ as in definition \ref{def:algo_basic}. Let $\vec a' \in \{\vec a_1, \dots, \vec a_n\}$ arbetrairy. As previously discussed, we know that $\sprod{\vec\mu}{\vec a_{j_1}} = \dots = \sprod{\vec\mu}{\vec a_{j_r}} =: d > 0$. Then $\sprod{\vec\mu}{\vec a'} \geq d$.
\end{lemma}
\begin{proof}
    Let $\vec b$ as in definition \ref{def:algo_basic}. The ray from the origin to $\vec b$ will hit the face with vertecies $\vec a_{j_1}, \dots, \vec a_{j_r}$ and this face lies in the affinie space passing through these vertecies. Because of lemma \ref{lemma:vertecies_are_basis} these are linearly independent and we can apply lemma \ref{lemma:affine_space}. Because of the convexity of the hull, the whole hull lies in $S_1$, in particular $\vec a' \in S_1$. If $\vec a' = \sum_{k=1}^r \lambda_k \vec a_{j_k}$ we now know by lemma \ref{lemma:affine_space} that $\sum_{k=1}^r \lambda_k \geq 1$. Now:
    $$\sprod{\vec\mu}{\vec a'} = \largesprod{\vec\mu}{\sum_{k=1}^r \lambda_k \vec a_{j_k}} = \sum_{k=1}^r\lambda_k \cdot \underbrace{\sprod{\vec\mu}{\vec a_{j_k}}}_d = d\cdot \underbrace{\sum_{k=1}^r\lambda_k}_{\geq 1} \geq d$$
\end{proof}

\subsubsection{The result is the global minimum}
Now let's compare our minimum to all other possible local minima. First we need to discuss the nature of all local minimums:

\begin{lemma}
    \label{lemma:local_minimum_faces_is_visible_from_origin}
    Let $k, \vec a_1, \dots, \vec a_n, U_1, \dots, U_n$ as in definition \ref{def:algo_basic}. Let $\vec\mu$ be some local minimum of $k$ thus $\vec\mu \in U_{j_1}\cap \dots \cap U_{j_r}$. Then the affine space containing $\vec a_{j_1}, \dots, \vec a_{j_r}$ will split the space in $S_0$ and $S_1$ in such a way that, $S_0$ contains the origin and $S_1$ the all points $\vec a_1, \dots, \vec a_n$.
\end{lemma}
\begin{proof}
    Proof by contradiction. Let $\vec a' \in \{\vec a_1, \dots, \vec a_n\}$ such that $\vec a' \in S_0$. Let $\vec a' := \sum_{k=1}^r \lambda_k \vec a_{j_k}$ e now know by lemma \ref{lemma:affine_space} that $\sum_{k=1}^r \lambda_k < 1$.
    Now:
    $$\sprod{\vec\mu}{\vec a'} = \largesprod{\vec\mu}{\sum_{k=1}^r \lambda_k \vec a_{j_k}} = \sum_{k=1}^r\lambda_k \cdot \underbrace{\sprod{\vec\mu}{\vec a_{j_k}}}_d = d\cdot \underbrace{\sum_{k=1}^r\lambda_k}_{< 1} < d$$
    But this means that $\vec\mu \notin U_{j_1}\cap \dots \cap U_{j_r}$ which yields the contradiction.
\end{proof}
Now the last stretch is actaully checking whether the found minimum is also the golbal minimum by showing that it is the smallest local minimum:
\begin{lemma}
    Let $\vec a_1, \dots, \vec a_n, \vec b, U_1, \dots, U_n, k$ as in definition \ref{def:algo_basic}. Let $\vec \mu$ be the result of the algorithm. We know that $\vec\mu \in U_{j_1} \cap \dots \cap U_{j_r}$. Let $\vec\mu'$ be an arbetrairy local minimum, and thus we know that $\vec\mu' \in U_{j'_1} \cap \dots \cap U_{j'_r}$. Then it will hold that $k(\vec\mu) \leq k(\vec\mu')$.
\end{lemma}
\begin{proof}
    Let $\lambda_1, \dots \lambda_r, \lambda'_1, \dots, \lambda'_r$ be such that $\vec b = \sum_{k=1}^{r}\lambda_k \vec a_{j_k} = \sum_{k=1}^{r}\lambda'_k \vec a_{j'_k}$. It is easy to see that because $\vec\mu \in U_{j_1} \cap \dots \cap U_{j_r}$ and $\vec\mu' \in U_{j'_1} \cap \dots \cap U_{j'_r}$ it will hold that $k(\vec\mu) = \sum_{k=1}^{r}\lambda_k$ and $k(\vec\mu') = \sum_{k=1}^{r}\lambda'_k$ respectively. The task is now to show that $\sum_{k=1}^{r}\lambda_k \leq \sum_{k=1}^{r}\lambda'_k$.

    Let $\vec p$ by the point, where the ray from the origin to $\vec b$ hits the convex hull of $\vec a_1, \dots, \vec a_n$. Because of lemma \ref{lemma:b_behind_hull} $\vec p = \gamma\vec b$ for $0 < \gamma \leq 1$. Let $\nu_1, \dots \nu_r, \nu'_1, \dots, \nu'_r$ such that be such that $\vec p = \sum_{k=1}^{r}\nu_k \vec a_{j_k} = \sum_{k=1}^{r}\nu'_k \vec a_{j'_k}$. By construction, $\vec p$ lies in the affine space that also contains $\vec a_{j_1}, \dots, \vec a_{j_r}$ thus $\sum_{k=1}^{r}\nu_k = 1$. Because $\vec p$ lies on the boundary of the convex hull and the affine space of $\vec a_{j'_1}, \dots, \vec a_{j'_r}$ will split $\R^n$ in $S_0$ and $S_1$ and by lemma \ref{lemma:local_minimum_faces_is_visible_from_origin} the whole convex hull lies in $S_1$, we know that $\vec p\in S_1$. By lemma \ref{lemma:affine_space} it follows that $\sum_{k=1}^{r}\nu'_k \geq 1$.

    Because $\vec b = \frac{1}{\gamma}\vec p$ we knwo that $\sum_{k=1}^{r}\lambda_k = \frac{1}{\gamma} \sum_{k=1}^{r}\nu_k$ and $\sum_{k=1}^{r}\lambda'_k = \frac{1}{\gamma} \sum_{k=1}^{r}\nu'_k$. This can be used in the final step:
    $$k(\vec\mu) = \sum_{k=1}^{r}\lambda_k = \frac{1}{\gamma} \sum_{k=1}^{r}\nu_k = \frac{1}{\gamma} \leq \frac{1}{\gamma} \sum_{k=1}^{r}\nu'_k = \sum_{k=1}^{r}\lambda'_k = k(\vec\mu')$$
\end{proof}

\subsection{Efficiently find the intersection of a ray and a convex hull}


\subsection{Walkthrough of an example}

\subsection{Discussion on slack variables}
Normally, an ILP is not given as a system of equations $A\vec x = \vec b$, where all entries in $\vec x$ are natural numbers, but a system of inequalities $A\vec x \leq \vec b$, where all entries in $\vec x$ are whole numbers. This notation does not make immediate sense, as it seams like we are comparing two vectors with a $\leq$-sign. What is ment is, that each component of $A\vec x$ must be smaller or equal the the corresponding component in $\vec b$. 

To convert from the the $\leq$-version to the $=$-version, you introduce another variable vector $\vec s$, resulting in linear system of equations:
$$A\vec x + \vec s = \vec b \qquad \vec x \in \N^n, \vec s \in\N^m$$
The variables in $\vec s$ are called slack variables. But instead of changing the equation we are dealing with, we can also just modify $A$ and $\vec x$ to get to the same result. If you just attach the vector $\vec s$ onto $\vec x$: $\vec x \mapsto (x_1, \dots, x_n, s_1, \dots, s_m)^\top \in \N^{n+m}$ and attach an identity matrix into $A$: $A \mapsto (A \mid\imat_m) \in \N^{m \times (n + m)}$. Thus, if we construct $A$ using the common method of slack variables, the columns of $A$ will include all possible unit vectors.

This fact hugely impacts the results we can expect from the algorithm. Let's think it through: As all unit vectors are included in the point cloud of which we create the convex hull with, the hyperplane $x_1 + \dots + x_n = 1$ will always be a face of the convex hull. It is clear geometrically, that any ray cast from the origin to some point $\vec b$ will always hat this plane first ‚Äì there cannot be a closer plane. Consequently, this face will be selected by the algorithm which means, the selected basis will be the standart basis.
Thus $A'$ will be some column permutation of the identity matrix. You can easily check that $\Delta(A')\opvec(1) = \vec 0$ and because all column sum in $A$ are postive, this means that $A^\top \opvec(1) > \vec 0$. Thus the algorithm could pick $\vec\mu = \opvec(1)$.

Why is this an issues now? $\opvec(1)$ represents the matrix $A$ itself without alterations. And as the algorithm spits out the best possible matrizes, we cannot imporve $A$ even further. Which means, that if we use slack variables, the resulting matrix is not improvable.