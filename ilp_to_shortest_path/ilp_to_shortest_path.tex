\chapter{Reducing special ILPs to Shortest Path}
\section{Introduction}
% I got this from the book
% Interesting insight
% Only constant column sum

\section{Solving shortest path using depth first search???}

\section{Preliminaries}
[Define a ILP?]

For us, it will be usefull to discuss matricies where all columns sum to the same number $\alpha$. Because we are in the framework of ILPs, we also use natural numbers as entries. So, we are interested in $A \in \N^{m \times n}$ if $\forall i \in [n]\colon \sum_{i=1}^{m}A_{ij} = \alpha$. These matricies have special properties when they are used in linear system of equations, which we will discover in the next lemma.

\begin{lemma}
    \label{lemma:ilp_pre1}
    Let $A\in \N^{m \times n}, A \neq \zeromat$ such that each column of $A$ sums up to the same number $\alpha \in \N$. Let $b \in \N^m$ arbitrary and consider the linear system of equation $Ax=b$, where $x \in \N^n$. Then, the following two statements are true

    \begin{enumerate}
        \item[(1)] For all solutions $x \in \R^n$ must hold that its components must always sum up to the same number $k$.
        \item[(2)] There only exist a solution of the components of $b$ sum up to a multiple of $\alpha$. This multiple turnes out to be $k \cdot \alpha$.
    \end{enumerate}
\end{lemma}

\begin{proof}
    $A \neq \zeromat \Rightarrow \alpha > 0$. Lets assume there exists a solution $x \in \R^n$ of the linear system of equation $Ax=b$.
    $$\sum_{i=1}^m b_i = \sum_{i=1}^{m}\sum_{j=1}^{n}A_{ij} x_j = \sum_{j=1}^{n}\underbrace{\sum_{i=1}^{m}A_{ij}}_\alpha x_j = \alpha \cdot \sum_{j=1}^{n}x_j$$
    Because $\alpha$ and $b_i$ are fixed and will not change dependent of $x$ and $\alpha \neq 0$, statement (1) immidiatly follows. Thus we have proven, if there exist a solution $x \in \N^n$, then it must hold that $\sum_{i=1}^{m}b_i = \alpha \cdot k$ which is the contraposition of (2).
\end{proof}
\section{Tropic polynomials???}
Let $c_{e_1, \dots, e_m}(p)$ be the coefficiant in $p \in S[q_1, \dots q_m]$ of the monomial $p_1^{e_1}\dots p_m^{e_m}$. 

\begin{lemma}
    \label{lemma:prem_trop_poly}
    Let $A \in \N^{m \times n}$ and $w \in \R^n$. Let
    $$f := \bigoplus_{i=1}^{n} \sprod{w}{\hat e_i}\odot  q_1^{(A\hat e_i)_1}\dots q_m^{(A\hat e_i)_m} \in \T[q_1, \dots, q_m]$$
    be a tropic polynomial. Then it will hold that for all $l \in \N$
    $$c_{e_1, \dots, e_m}(f^{\odot l}) = \min\left\{\sprod{w}{v} \mid v \in \N^n, \sum_{i=0}^{n}v_i = l, Av=\smat{e_1\\\vdots\\e_m}\right\}$$
    Where $\min\emptyset:=\infty$.
\end{lemma}

\begin{proof}
    We will prove it by induction over $l$.
    \begin{itemize}
        \item[$l=0$:] $f^{\odot0} = 0$. There exists only one vector in $v \in \N^n$ such that $\sum_{i=0}^{n}v_i = 0$, which is the zero vector $\vec 0$. It is also clear that $\sprod{w}{\vec 0}= 0$ and $A\vec0=0$. So
        $$
            \min\left\{\sprod{w}{v} \mid v \in \N^n, \sum_{i=0}^{n}v_i = 0, Av=\smat{e_1\\\vdots\\e_m}\right\} = \begin{cases}
                0 &\textrm{if}\quad e_1, \dots, e_m = 0\\
                \infty &\textrm{else}
            \end{cases}
            \stackrel{\checkmark}{=}c_{e_1, \dots, e_m}(0)
        $$
        \item[$l=1$:] Because $\hat e_1, \dots, \hat e_n$ are all the vectors in $\N^n$, such that their components add up to 1, the equality is true by construction.
        \item[$l>1$:] Let $e := (e_1, \dots, e_m)^\top \in \N^m$
        \begin{align*}
            c_{e_1, \dots, e_m}(f^{\odot l}) &= c_{e_1, \dots, e_m}(f^{\odot (l-1)}\odot f) = \bigoplus_{\substack{f, g \in \N^m\\f+g=e}} c_{f_1, \dots, f_m}(f^{\odot (l-1)}) \odot c_{g_1, \dots, g_m}(f)\\
            &= \bigoplus_{\substack{f, g \in \N^m\\f+g=e}} \left(\bigoplus_{\substack{x \in \N^n\\\sum_{i=1}^{n}x_i = l-1\\Ax=f}}\sprod{w}{x}\right) \odot \left(\bigoplus_{\substack{y \in \N^n\\\sum_{i=1}^{n}y_i = 1\\Ay=g}}\sprod{w}{y}\right)\\
            &= \bigoplus_{\substack{f, g \in \N^m\\f+g=e}}\bigoplus_{\substack{x, y \in \N^n\\\sum_{i=1}^{n}x_i = l-1\\\sum_{i=1}^{n}y_i = 1\\Ax=f, Ay=g}} \underbrace{\sprod{w}{x} \odot \sprod{x}{y}}_{\sprod{w}{x+y} =: \sprod{x}{v}} = \bigoplus_{\substack{f, g \in \N^m\\f+g=e}}\bigoplus_{\substack{v \in \N^n\\\sum_{i=1}^{n}v = l\\Av=f+g=e}} \sprod{w}{v}\\
            &= \bigoplus_{\substack{v \in \N^n\\\sum_{i=1}^{n}v = l\\Av=e}} \sprod{w}{v} = \min\left\{\sprod{w}{v} \mid v \in \N^n, \sum_{i=0}^{n}v_i = l, Av=e\right\}
        \end{align*} 
    \end{itemize}
\end{proof}

\begin{theorem}
    Let $\min \stackrel{!}{=} w^\top x$ s.t. $Ax=b$ an ILP, such that all columns of $A$ sum up to the same number $\alpha \in \N^*$. We wat the ILP, to have solutions, so $k = \frac{1}{\alpha}\sum_{i=1}^{m}b_i \in \N$ exists. Let $f \in \T[q_1, \dots, q_m]$ be like in Lemma \ref{lemma:prem_trop_poly}, so 
    $$f := \bigoplus_{i=1}^{n} \sprod{w}{\hat e_i}\odot  q_1^{(A\hat e_i)_1}\dots q_m^{(A\hat e_i)_m}$$
    Then $c_{b_1, \dots, b_m}(f^{\odot k})$ solves the ILP 
\end{theorem}

\begin{proof}
    Because of Lemma \ref{lemma:ilp_pre1}, for all solutions $x \in \N^n$ of $Ax=b$ will hold that $\sum_{i=1}^{n}x_i = k$. No it is just a matter of rewriting the solution of the ILP and applying Lemma \ref{lemma:prem_trop_poly}.
    \begin{align*}
        \min\{\sprod{w}{x} \mid x \in \N^n, Ax=b\} &\stackrel{\ref{lemma:ilp_pre1}}{=} \min\left\{\sprod{w}{x} \mid x \in \N^n, \sum_{i=1}^{n}x_i = k , Ax=b\right\}\\
        &\stackrel{\ref{lemma:prem_trop_poly}}{=} c_{b_1, \dots, b_m}(f^{\odot k})
    \end{align*}
\end{proof}

\section{Solving ILPs by Shortest Path}
We want to be able to solve an ILP by using the shortest path framework, so the first task is to construct a directect weighted graph $G = (V, E)$ based on an ILP ($\sprod{w}{x} \stackrel{!}{=} \min$ s.t. $Ax=b$). Again we only look at matricies, in which all columns sum up to the same number $\alpha \in \N^*$. Let $k := \frac{1}{\alpha} \sum_{i=1}^{n}x_i \in \N$. Because of Lemma \ref{lemma:ilp_pre1}, $k$ exists. Now we are ready to set the vertecies:
$$V := \left\{Ax \mid x \in \N^n, \sum_{i=1}^{n} x_i \leq k \right\}$$ 
If $Ax=b$ has solutions, we immidiatly see that $b \in V$, because the components of all solutions add up to exactly $k$. We also note that $\vec 0 = A\vec 0 \in V$. Now we can define the edges:
$$E := \{(v, u) \mid v, u \in V, u = v + A\hat e_i, i \in [n]\}$$
In other words, an edge exists, if underlying vectors only differ by an unit vector.

The resulting graph will be a tree with $\vec 0$ as the root. We will define the weight as follow:
$$w(v, v + A\hat e_i) := \sprod{w}{\hat e_i}$$
The nodes with distance $l$ will be the the nodes $Ax$ where $\sum_{i=1}^{n}x_i = l$. So the tree will have depth $k+1$. So the shortest path between $\vec 0$ and $b$ will be 

$$\min\left\{\sum_{l=1}^{k} \sprod{w}{\hat e^{(l)}} \mid \sum_{l=1}^{k}\hat Ae^{(l)} = b, \hat e^{(l)} \in \{\hat e_{1}, \dots, \hat e_n\}\right\}$$

By using the liniarity of the scalar product and the matrix multiplication and than setting $x := \sum_{l=1}^{k}\hat e^{(l)}$ this is equal to 
$$\min\left\{\sprod{w}{x} \mid x \in \N^n, \sum_{i=1}^{n}x_i = k , Ax=b\right\} \stackrel{\ref{lemma:ilp_pre1}}{=} \min\{\sprod{w}{x} \mid x \in \N^n, Ax=b\}$$
which solves th ILP.\qed

\section{Immediate optimisation ideas}
% Nicht mehrere Pfade frü einen Vektor
% Wenn einträge zu groß werden discarden
% Kinderlose löschen
